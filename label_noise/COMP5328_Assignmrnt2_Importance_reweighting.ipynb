{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and  pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def loaddata():\n",
    "    dataset_mnist = np.load('mnist_dataset.npz')\n",
    "    dataset_cifar = np.load('cifar_dataset.npz')\n",
    "\n",
    "    Xtr_mnist = dataset_mnist['Xtr']\n",
    "    Str_mnist = dataset_mnist['Str']\n",
    "    Xts_mnist = dataset_mnist['Xts']\n",
    "    Yts_mnist = dataset_mnist['Yts']\n",
    "\n",
    "    Xtr_cifar = dataset_cifar['Xtr']\n",
    "    Str_cifar = dataset_cifar['Str']\n",
    "    Xts_cifar = dataset_cifar['Xts']\n",
    "    Yts_cifar = dataset_cifar['Yts']\n",
    "\n",
    "    return Xtr_mnist, Str_mnist, Xts_mnist, Yts_mnist, Xtr_cifar, Str_cifar, Xts_cifar, Yts_cifar\n",
    "\n",
    "\n",
    "def standard(x_train, x_test):\n",
    "    '''\n",
    "    Note:\n",
    "         Use the std and mean from the training data and implement\n",
    "         it to the validation data and test data\n",
    "    RETRUN:\n",
    "        Flattened and standard(Z-score normalization) training data\n",
    "        The features size is 3072 or 784\n",
    "    '''\n",
    "    std = np.std(x_train, keepdims=True)\n",
    "    mean = np.mean(x_train, keepdims=True)\n",
    "    x_train = (x_train-mean)/std\n",
    "    x_test = (x_test-mean)/std\n",
    "    return x_train, x_test\n",
    "\n",
    "\n",
    "def split_data(x_train, y_train, part=0.8):\n",
    "    all_index = np.arange(y_train.shape[0])\n",
    "    np.random.shuffle(all_index)\n",
    "    \n",
    "    train_data_size = int(y_train.shape[0] * part)\n",
    "    \n",
    "    train_index = all_index[0:train_data_size]\n",
    "    val_index= all_index[train_data_size:]\n",
    "    \n",
    "    \n",
    "    x_vali = x_train[val_index]\n",
    "    y_vali = y_train[val_index]\n",
    "\n",
    "    x_train = x_train[train_index]\n",
    "    y_train = y_train[train_index]\n",
    "\n",
    "    return x_train, y_train, x_vali, y_vali,train_index\n",
    "\n",
    "\n",
    "def one_hot_coding(y_train, y_test):\n",
    "    '''\n",
    "    Encode label to one-hot catogory\n",
    "    '''\n",
    "    y_train = to_categorical(y_train, num_classes=2)\n",
    "    y_test = to_categorical(y_test, num_classes=2)\n",
    "    return y_train, y_test\n",
    "\n",
    "def compute_acc(y_ture, y_pred):\n",
    "    acc = np.sum(y_ture == np.argmax(y_pred, axis=1))/len(y_ture)\n",
    "    return round(acc, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN model and estimate $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_cnn_model(x_train, y_train, x_vali, y_vali, TYPE='with_noise',\n",
    "                    lr=0.0001, decay=0.000001, batchsize=256, epochs=1):\n",
    "    # data is mnist\n",
    "    if x_train.shape[3] == 1:\n",
    "        model = Sequential([\n",
    "            Conv2D(64, (2, 2), use_bias=True, padding='same', activation='relu',input_shape=(28,28,1)),\n",
    "            MaxPooling2D(pool_size=(2, 2)),\n",
    "            Conv2D(64, (2, 2), use_bias=True, padding='same', activation='relu'),\n",
    "            Flatten(),\n",
    "            Dense(1000, activation='relu', use_bias=True),\n",
    "            Dense(500, activation='relu', use_bias=True),\n",
    "            Dense(2, activation=tf.nn.softmax)\n",
    "        ])\n",
    "        \n",
    "    # data is cifar\n",
    "    else:\n",
    "        model = Sequential([\n",
    "            Conv2D(64, (2, 2), use_bias=True, padding='same', activation='relu',input_shape=(32,32,3)),\n",
    "            MaxPooling2D(pool_size=(2, 2)),\n",
    "            Conv2D(64, (2, 2), use_bias=True, padding='same', activation='relu'),\n",
    "            Flatten(),\n",
    "            Dense(1000, activation='relu', use_bias=True),\n",
    "            Dense(500, activation='relu', use_bias=True),\n",
    "            Dense(2, activation=tf.nn.softmax)\n",
    "        ])\n",
    "\n",
    "    adm = optimizers.Adam(lr=lr, decay=decay)\n",
    "    if TYPE == 'with_noise':\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=adm,\n",
    "                      metrics=['accuracy'])\n",
    "        history = model.fit(x_train, y_train, batch_size=batchsize, \\\n",
    "                        epochs=epochs)\n",
    "    elif TYPE == 'without_noise':\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=adm,\n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "        history = model.fit(x_train, y_train, batch_size=batchsize,sample_weight = weights.flatten(), \\\n",
    "                        epochs=epochs)\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def estimateBeta(S,prob,rho0,rho1):\n",
    "    n = len(S)\n",
    "    beta = np.zeros((n,1))\n",
    "    for i in range(n):\n",
    "        if S[i]==1:\n",
    "            beta[i] = (prob[i][1]-rho0)/((1-rho0-rho1)*prob[i][1]+1e-5)\n",
    "        else:\n",
    "            beta[i] = (prob[i][0]-rho1)/((1-rho0-rho1)*(prob[i][0])+1e-5)\n",
    "\n",
    "    return beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (10000, 28, 28, 1), y_train (10000, 2) \n",
      "Xts_mnist (2000, 28, 28, 1), y_test (2000, 2)\n"
     ]
    }
   ],
   "source": [
    "Xtr_mnist, Str_mnist, Xts_mnist, Yts_mnist, Xtr_cifar, Str_cifar, Xts_cifar, Yts_cifar = loaddata()\n",
    "Xtr_mnist,Xts_mnist = standard(Xtr_mnist,Xts_mnist)\n",
    "Xtr_mnist = Xtr_mnist.reshape(Xtr_mnist.shape[0], 28, 28,1)\n",
    "Xts_mnist = Xts_mnist.reshape(Xts_mnist.shape[0], 28, 28,1)\n",
    "y_train_o, y_test = one_hot_coding(Str_mnist,Yts_mnist)\n",
    "\n",
    "print('x_train {}, y_train {} \\nXts_mnist {}, y_test {}'.format(\n",
    "    Xtr_mnist.shape,y_train_o.shape,Xts_mnist.shape,y_test.shape))\n",
    "\n",
    "rho0 = 0.2\n",
    "rho1 = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training without importance reweighting for Fashion-MNIST\n",
    "Use the parameters we got from cross validation step   \n",
    "Randomly select 8000 samples from training set, use these to train models   \n",
    "Calculate the probabilities which would be used to calculate weight $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 0.6784 - acc: 0.5639\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6539 - acc: 0.6102\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6387 - acc: 0.6405\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6273 - acc: 0.6410\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6187 - acc: 0.6420\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6115 - acc: 0.6571\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 0.6062 - acc: 0.6619\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.6024 - acc: 0.6735\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.5993 - acc: 0.6738\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.5970 - acc: 0.6780\n",
      "2000/2000 [==============================] - 0s 56us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 0.6814 - acc: 0.5461\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6549 - acc: 0.6016\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6397 - acc: 0.6281\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6301 - acc: 0.6325\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6220 - acc: 0.6330\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6154 - acc: 0.6448\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6104 - acc: 0.6544\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6066 - acc: 0.6626\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6038 - acc: 0.6666\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6016 - acc: 0.6741\n",
      "2000/2000 [==============================] - 0s 66us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 0.6675 - acc: 0.6011\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6496 - acc: 0.6031\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6370 - acc: 0.6275\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6284 - acc: 0.6312\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6208 - acc: 0.6379\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6154 - acc: 0.6468\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6113 - acc: 0.6574\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6084 - acc: 0.6587\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6063 - acc: 0.6685\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6041 - acc: 0.6673\n",
      "2000/2000 [==============================] - 0s 62us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.6824 - acc: 0.5411\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6560 - acc: 0.6021\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6417 - acc: 0.6316\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6310 - acc: 0.6344\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6226 - acc: 0.6350\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6171 - acc: 0.6491\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6117 - acc: 0.6504\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6081 - acc: 0.6659\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6052 - acc: 0.6715\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6033 - acc: 0.6692\n",
      "2000/2000 [==============================] - 0s 76us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 0.6793 - acc: 0.5672\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6554 - acc: 0.6068\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6424 - acc: 0.6305\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 0.6322 - acc: 0.6311\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6241 - acc: 0.6366\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6177 - acc: 0.6407\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6130 - acc: 0.6614\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6091 - acc: 0.6576\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6063 - acc: 0.6673\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6041 - acc: 0.6776\n",
      "2000/2000 [==============================] - 0s 74us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 107us/step - loss: 0.6887 - acc: 0.5319\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6636 - acc: 0.6016\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6474 - acc: 0.6240\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6389 - acc: 0.6306\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6310 - acc: 0.6303\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6249 - acc: 0.6398\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6185 - acc: 0.6386\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6142 - acc: 0.6569\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6106 - acc: 0.6566\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6080 - acc: 0.6644\n",
      "2000/2000 [==============================] - 0s 82us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 0.6687 - acc: 0.6011\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6491 - acc: 0.6081\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6355 - acc: 0.6331\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6252 - acc: 0.6358\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.6176 - acc: 0.6445\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6109 - acc: 0.6500\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6061 - acc: 0.6628\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6026 - acc: 0.6728\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6003 - acc: 0.6774\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5987 - acc: 0.6726\n",
      "2000/2000 [==============================] - 0s 88us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 0.6798 - acc: 0.5566\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6565 - acc: 0.6036\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6431 - acc: 0.6332\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6334 - acc: 0.6334\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6258 - acc: 0.6376\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6191 - acc: 0.6413\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6151 - acc: 0.6492\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6113 - acc: 0.6535\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6089 - acc: 0.6662\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6059 - acc: 0.6660\n",
      "2000/2000 [==============================] - 0s 105us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 0.6819 - acc: 0.5494\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6548 - acc: 0.6036\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6393 - acc: 0.6282\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6302 - acc: 0.6373\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6217 - acc: 0.6392\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6154 - acc: 0.6549\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6109 - acc: 0.6537\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6074 - acc: 0.6640\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6049 - acc: 0.6663\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6028 - acc: 0.6721\n",
      "2000/2000 [==============================] - 0s 101us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 0.6754 - acc: 0.5822\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6534 - acc: 0.6071\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6385 - acc: 0.6314\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6277 - acc: 0.6317\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6195 - acc: 0.6376\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6135 - acc: 0.6601\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6087 - acc: 0.6614\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6059 - acc: 0.6736\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6044 - acc: 0.6672\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6022 - acc: 0.6737\n",
      "2000/2000 [==============================] - 0s 117us/step\n"
     ]
    }
   ],
   "source": [
    "for_flip_rate_1_l = []\n",
    "for_flip_rate_0_l = []\n",
    "acc_l = []\n",
    "run_times = 10\n",
    "x_train_x_results = []\n",
    "for i in range(run_times):\n",
    "    x_train, y_train, x_vali, y_vali,split_index = split_data(Xtr_mnist,y_train_o,part=0.8)\n",
    "    model_noise,history_noise = train_cnn_model(x_train, y_train, x_vali, y_vali, TYPE='with_noise',\n",
    "                        lr=0.00001, decay=0.000001, batchsize=1024, epochs=10)\n",
    "    \n",
    "    train_x_result = model_noise.predict(Xtr_mnist)\n",
    "\n",
    "    for_flip_rate_1_l.append(np.min(train_x_result[:,0]))\n",
    "    for_flip_rate_0_l.append(np.min(train_x_result[:,1]))\n",
    "\n",
    "    l,a=model_noise.evaluate(Xts_mnist,y_test)\n",
    "    \n",
    "    acc_l.append(a)\n",
    "    x_train_x_results.append(train_x_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8436\n",
      "0.026624988262908204\n"
     ]
    }
   ],
   "source": [
    "# print(for_flip_rate_0_l)\n",
    "# print(for_flip_rate_1_l)\n",
    "\n",
    "# print(acc_l)\n",
    "\n",
    "print(sum(acc_l)/len(acc_l))\n",
    "print(np.array(acc_l).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average probabilities and calculate weight $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_train_x_results)):\n",
    "    if i == 0:\n",
    "        total_x_result = np.copy(x_train_x_results[i])\n",
    "    else:\n",
    "        total_x_result += x_train_x_results[i]\n",
    "train_x_result = total_x_result/(run_times)\n",
    "\n",
    "\n",
    "all_weights = estimateBeta(Str_mnist, train_x_result, rho0, rho1)\n",
    "for i in range(len(all_weights)):\n",
    "    if all_weights[i] < 0:\n",
    "        all_weights[i] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with importance reweighting method for Fashion-MNIST\n",
    "Randomly select 8000 samples from training set, use these to train models   \n",
    "Calculate accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 0.6643 - acc: 0.5864\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6036 - acc: 0.6281\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5529 - acc: 0.6450\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.5076 - acc: 0.6526\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.4684 - acc: 0.6577\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.4345 - acc: 0.6630\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4064 - acc: 0.6650\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.3838 - acc: 0.6679\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3665 - acc: 0.6709\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.3521 - acc: 0.6713\n",
      "2000/2000 [==============================] - 0s 111us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 128us/step - loss: 0.6720 - acc: 0.6092\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6113 - acc: 0.6337\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5620 - acc: 0.6481\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.5182 - acc: 0.6551\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4782 - acc: 0.6637\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.4419 - acc: 0.6651\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.4112 - acc: 0.6687\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.3861 - acc: 0.6700\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.3661 - acc: 0.6721\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.3519 - acc: 0.6686\n",
      "2000/2000 [==============================] - 0s 121us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 131us/step - loss: 0.6761 - acc: 0.6205\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6163 - acc: 0.6446\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5658 - acc: 0.6496\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5220 - acc: 0.6589\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4823 - acc: 0.6621\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.4466 - acc: 0.6635\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4156 - acc: 0.6688\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.3901 - acc: 0.6694\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3696 - acc: 0.6705\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3541 - acc: 0.6706\n",
      "2000/2000 [==============================] - 0s 127us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 134us/step - loss: 0.6631 - acc: 0.6015\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5926 - acc: 0.6320\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5349 - acc: 0.6447\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4850 - acc: 0.6576\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4433 - acc: 0.6669\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.4085 - acc: 0.6696\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 35us/step - loss: 0.3811 - acc: 0.6720\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 36us/step - loss: 0.3609 - acc: 0.6716\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3459 - acc: 0.6715\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3360 - acc: 0.6718\n",
      "2000/2000 [==============================] - 0s 133us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 0.6564 - acc: 0.5605\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5915 - acc: 0.6290\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.5407 - acc: 0.6441\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.4971 - acc: 0.6596\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4601 - acc: 0.6629\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4289 - acc: 0.6631\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4035 - acc: 0.6662\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3819 - acc: 0.6665\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3657 - acc: 0.6678\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3532 - acc: 0.6664\n",
      "2000/2000 [==============================] - 0s 142us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 145us/step - loss: 0.6738 - acc: 0.6142\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6169 - acc: 0.6326\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5671 - acc: 0.6459\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5210 - acc: 0.6561\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.4790 - acc: 0.6626\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4424 - acc: 0.6669\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4108 - acc: 0.6705\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3856 - acc: 0.6719\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3661 - acc: 0.6732\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3520 - acc: 0.6734\n",
      "2000/2000 [==============================] - 0s 152us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 148us/step - loss: 0.6786 - acc: 0.6084\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6194 - acc: 0.6327\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5683 - acc: 0.6418\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5223 - acc: 0.6531\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4807 - acc: 0.6627\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4439 - acc: 0.6643\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4122 - acc: 0.6699\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 35us/step - loss: 0.3854 - acc: 0.6724\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 36us/step - loss: 0.3643 - acc: 0.6730\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.3486 - acc: 0.6726\n",
      "2000/2000 [==============================] - 0s 156us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 151us/step - loss: 0.6547 - acc: 0.6019\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5910 - acc: 0.6293\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5403 - acc: 0.6411\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4971 - acc: 0.6519\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.4598 - acc: 0.6583\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4281 - acc: 0.6640\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4018 - acc: 0.6655\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3806 - acc: 0.6676\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3647 - acc: 0.6687\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3527 - acc: 0.6686\n",
      "2000/2000 [==============================] - 0s 174us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 0.6640 - acc: 0.5982\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5855 - acc: 0.6304\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5282 - acc: 0.6466\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4806 - acc: 0.6595\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4405 - acc: 0.6641\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4070 - acc: 0.6676\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3807 - acc: 0.6690\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3612 - acc: 0.6682\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3468 - acc: 0.6681\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.3376 - acc: 0.6697\n",
      "2000/2000 [==============================] - 0s 189us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 0.6720 - acc: 0.6274\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6072 - acc: 0.6546\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5554 - acc: 0.6517\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5096 - acc: 0.6581\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4699 - acc: 0.6617\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4356 - acc: 0.6650\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 0.4055 - acc: 0.6703\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 35us/step - loss: 0.3822 - acc: 0.6710\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.3650 - acc: 0.6680\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3505 - acc: 0.6703\n",
      "2000/2000 [==============================] - 0s 181us/step\n"
     ]
    }
   ],
   "source": [
    "acc_l_without = []\n",
    "for i in range(run_times):\n",
    "    x_train, y_train, x_vali, y_vali,split_index = split_data(Xtr_mnist,y_train_o,part=0.8)\n",
    "    \n",
    "    weights = all_weights[split_index]\n",
    "    \n",
    "    model, history = train_cnn_model(x_train, y_train, x_vali, y_vali, TYPE='without_noise',\n",
    "                        lr=0.00001, decay=0.000001, batchsize=1024, epochs=10)\n",
    "    \n",
    "#     model.evaluate(Xts_mnist,y_test)\n",
    "    l,a=model.evaluate(Xts_mnist,y_test)\n",
    "    acc_l_without.append(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate average accuracy and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.925\n",
      "0.003981205847478865\n"
     ]
    }
   ],
   "source": [
    "# print(acc_l_without)\n",
    "\n",
    "print(sum(acc_l_without)/len(acc_l_without))\n",
    "print(np.array(acc_l_without).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cifar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (10000, 32, 32, 3), y_train (10000, 2) \n",
      "Xts_cifar (2000, 32, 32, 3), y_test (2000, 2)\n"
     ]
    }
   ],
   "source": [
    "Xtr_cifar,Xts_cifar = standard(Xtr_cifar,Xts_cifar)\n",
    "\n",
    "Xtr_cifar = Xtr_cifar.reshape(Xtr_cifar.shape[0], 32,32,3)\n",
    "Xts_cifar = Xts_cifar.reshape(Xts_cifar.shape[0], 32,32,3)\n",
    "\n",
    "y_train_o, y_test = one_hot_coding(Str_cifar,Yts_cifar)\n",
    "\n",
    "print('x_train {}, y_train {} \\nXts_cifar {}, y_test {}'.format(\n",
    "    Xtr_cifar.shape,y_train_o.shape,Xts_cifar.shape,y_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training without importance reweighting for Cifar\n",
    "Use the parameters we got from cross validation step   \n",
    "Randomly select 8000 samples from training set, use these to train models   \n",
    "Calculate the probabilities which would be used to calculate weight $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 2s 247us/step - loss: 0.6855 - acc: 0.5765\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.6444 - acc: 0.6241\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.6350 - acc: 0.6396\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.6207 - acc: 0.6659\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.6119 - acc: 0.6707\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.6050 - acc: 0.6824\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.6003 - acc: 0.6808\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.5945 - acc: 0.6870\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.5838 - acc: 0.7009\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.5741 - acc: 0.7136\n",
      "2000/2000 [==============================] - 0s 81us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 0.6928 - acc: 0.5654\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.6486 - acc: 0.6267\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.6344 - acc: 0.6431\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.6241 - acc: 0.6580\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.6161 - acc: 0.6715\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.6069 - acc: 0.6778\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.5984 - acc: 0.6870\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.5887 - acc: 0.7034\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.5788 - acc: 0.7089\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.5698 - acc: 0.7211\n",
      "2000/2000 [==============================] - 0s 82us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 126us/step - loss: 0.6631 - acc: 0.6036\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.6314 - acc: 0.6481\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.6185 - acc: 0.6673\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.6088 - acc: 0.6756\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.5987 - acc: 0.6838\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.5875 - acc: 0.6985\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.5765 - acc: 0.7119\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.5656 - acc: 0.7305\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.5518 - acc: 0.7388\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.5381 - acc: 0.7536\n",
      "2000/2000 [==============================] - 0s 92us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 129us/step - loss: 0.6758 - acc: 0.5823\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.6346 - acc: 0.6380\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.6211 - acc: 0.6616\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.6110 - acc: 0.6762\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 0.6013 - acc: 0.6834\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.5915 - acc: 0.6926\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.5838 - acc: 0.7020\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.5759 - acc: 0.7075\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.5675 - acc: 0.7190\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.5524 - acc: 0.7378\n",
      "2000/2000 [==============================] - 0s 90us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 129us/step - loss: 0.6576 - acc: 0.6031\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.6297 - acc: 0.6473\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.6153 - acc: 0.6631\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.6027 - acc: 0.6820\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.5928 - acc: 0.6900\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.5882 - acc: 0.6951\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.5708 - acc: 0.7180\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.5588 - acc: 0.7294\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.5450 - acc: 0.7451\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.5322 - acc: 0.7544\n",
      "2000/2000 [==============================] - 0s 117us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 0.6721 - acc: 0.5940\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 0.6353 - acc: 0.6365\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 0.6211 - acc: 0.6606\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 0.6167 - acc: 0.6631\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.6091 - acc: 0.6716\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 0.5983 - acc: 0.6837\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.5887 - acc: 0.6978\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.5776 - acc: 0.7101\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.5661 - acc: 0.7199\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.5555 - acc: 0.7348\n",
      "2000/2000 [==============================] - 0s 116us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 0.6511 - acc: 0.6235\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.6265 - acc: 0.6586\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.6125 - acc: 0.6735\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 0.6015 - acc: 0.6820\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.5946 - acc: 0.6851\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.5873 - acc: 0.7022\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.5740 - acc: 0.7121\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 0.5634 - acc: 0.7250\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.5519 - acc: 0.7361\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.5469 - acc: 0.7355\n",
      "2000/2000 [==============================] - 0s 122us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 144us/step - loss: 0.6601 - acc: 0.5959\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.6339 - acc: 0.6463\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.6184 - acc: 0.6573\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.6095 - acc: 0.6766\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.6000 - acc: 0.6875\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.5893 - acc: 0.7000\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.5777 - acc: 0.7134\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.5682 - acc: 0.7209\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.5565 - acc: 0.7282\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.5443 - acc: 0.7454\n",
      "2000/2000 [==============================] - 0s 125us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 144us/step - loss: 0.6527 - acc: 0.6190\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.6257 - acc: 0.6571\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 0.6133 - acc: 0.6774\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 0.6011 - acc: 0.6840\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.5905 - acc: 0.7003\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.5795 - acc: 0.7114\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.5738 - acc: 0.7179\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.5644 - acc: 0.7246\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.5468 - acc: 0.7388\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.5342 - acc: 0.7539\n",
      "2000/2000 [==============================] - 0s 139us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 147us/step - loss: 0.6683 - acc: 0.5955\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.6407 - acc: 0.6318\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 0.6264 - acc: 0.6515\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 0.6159 - acc: 0.6651\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.6055 - acc: 0.6866\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 0.5956 - acc: 0.6969\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.5866 - acc: 0.7001\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.5768 - acc: 0.7158\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.5652 - acc: 0.7245\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.5517 - acc: 0.7397\n",
      "2000/2000 [==============================] - 0s 153us/step\n"
     ]
    }
   ],
   "source": [
    "for_flip_rate_1_l = []\n",
    "for_flip_rate_0_l = []\n",
    "acc_l = []\n",
    "run_times = 10\n",
    "x_train_x_results = []\n",
    "\n",
    "for i in range(run_times):\n",
    "    x_train, y_train, x_vali, y_vali,split_index = split_data(Xtr_cifar,y_train_o,part=0.8)\n",
    "    model_noise,history_noise = train_cnn_model(x_train, y_train, x_vali, y_vali, TYPE='with_noise',\n",
    "                        lr=0.00005, decay=0.000001, batchsize=1024, epochs=10)\n",
    "    \n",
    "    train_x_result = model_noise.predict(Xtr_cifar)\n",
    "\n",
    "    for_flip_rate_1_l.append(np.min(train_x_result[:,0]))\n",
    "    for_flip_rate_0_l.append(np.min(train_x_result[:,1]))\n",
    "\n",
    "    l,a=model_noise.evaluate(Xts_cifar,y_test)\n",
    "    \n",
    "    acc_l.append(a)\n",
    "    x_train_x_results.append(train_x_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8109\n",
      "0.04070245692829858\n"
     ]
    }
   ],
   "source": [
    "# print(for_flip_rate_0_l)\n",
    "# print(for_flip_rate_1_l)\n",
    "\n",
    "# print(acc_l)\n",
    "\n",
    "print(sum(acc_l)/len(acc_l))\n",
    "print(np.array(acc_l).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average probabilities and calculate weight $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_train_x_results)):\n",
    "    if i == 0:\n",
    "        total_x_result = np.copy(x_train_x_results[i])\n",
    "    else:\n",
    "        total_x_result += x_train_x_results[i]\n",
    "        \n",
    "train_x_result = total_x_result/(run_times)\n",
    "\n",
    "all_weights = estimateBeta(Str_cifar, train_x_result, rho0, rho1)\n",
    "for i in range(len(all_weights)):\n",
    "    if all_weights[i] < 0:\n",
    "        all_weights[i] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with importance reweighting method for Cifar\n",
    "Randomly select 8000 samples from training set, use these to train models   \n",
    "Calculate accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 153us/step - loss: 0.5953 - acc: 0.5703\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.4995 - acc: 0.6206\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.4642 - acc: 0.6323\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.4354 - acc: 0.6438\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.4169 - acc: 0.6555\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.4000 - acc: 0.6594\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 0.3852 - acc: 0.6694\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.3740 - acc: 0.6771\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.3626 - acc: 0.6818\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.3538 - acc: 0.6896\n",
      "2000/2000 [==============================] - 0s 134us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 155us/step - loss: 0.5602 - acc: 0.5779\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.4676 - acc: 0.6277\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 0.4393 - acc: 0.6424\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.4229 - acc: 0.6473\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.4083 - acc: 0.6584\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.3963 - acc: 0.6643\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.3821 - acc: 0.6693\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.3708 - acc: 0.6748\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.3629 - acc: 0.6816\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.3558 - acc: 0.6863\n",
      "2000/2000 [==============================] - 0s 148us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 0.5696 - acc: 0.5701\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.4643 - acc: 0.6365\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.4329 - acc: 0.6469\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.4157 - acc: 0.6569\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.4016 - acc: 0.6590\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.3851 - acc: 0.6698\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 0.3744 - acc: 0.6793\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.3673 - acc: 0.6785\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.3593 - acc: 0.6866\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.3464 - acc: 0.6925\n",
      "2000/2000 [==============================] - 0s 175us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 162us/step - loss: 0.6037 - acc: 0.5677\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.5022 - acc: 0.6174\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.4653 - acc: 0.6377\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.4391 - acc: 0.6452\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.4213 - acc: 0.6525\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.4031 - acc: 0.6635\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.3921 - acc: 0.6671\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.3803 - acc: 0.6760\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.3686 - acc: 0.6837\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.3583 - acc: 0.6871\n",
      "2000/2000 [==============================] - 0s 167us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 0.5813 - acc: 0.5676\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.4747 - acc: 0.6276\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.4390 - acc: 0.6449\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.4196 - acc: 0.6513\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.4016 - acc: 0.6629\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.3872 - acc: 0.6657\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.3778 - acc: 0.6722\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.3647 - acc: 0.6839\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 0.3541 - acc: 0.6870\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 0.3478 - acc: 0.6926\n",
      "2000/2000 [==============================] - 0s 167us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 170us/step - loss: 0.6042 - acc: 0.5704\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.4975 - acc: 0.6186\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.4560 - acc: 0.6420\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.4351 - acc: 0.6417\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.4150 - acc: 0.6557\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.4023 - acc: 0.6598\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.3898 - acc: 0.6661\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 0.3785 - acc: 0.6738\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.3682 - acc: 0.6790\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 0.3621 - acc: 0.6836\n",
      "2000/2000 [==============================] - 0s 188us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 0.5920 - acc: 0.5607\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.4985 - acc: 0.6261\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.4588 - acc: 0.6371\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.4405 - acc: 0.6436\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.4209 - acc: 0.6521\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.4079 - acc: 0.6565\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.3964 - acc: 0.6660\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.3843 - acc: 0.6705\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.3757 - acc: 0.6715\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.3636 - acc: 0.6824\n",
      "2000/2000 [==============================] - 0s 194us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.5926 - acc: 0.5718\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.4857 - acc: 0.6220\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.4473 - acc: 0.6381\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.4258 - acc: 0.6492\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.4111 - acc: 0.6541\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.3965 - acc: 0.6644\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.3843 - acc: 0.6655\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.3731 - acc: 0.6751\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.3625 - acc: 0.6820\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.3587 - acc: 0.6852\n",
      "2000/2000 [==============================] - 0s 196us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 180us/step - loss: 0.5847 - acc: 0.5649\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.4777 - acc: 0.6250\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.4501 - acc: 0.6354\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.4268 - acc: 0.6506\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.4107 - acc: 0.6549\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.3981 - acc: 0.6654\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.3853 - acc: 0.6669\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.3739 - acc: 0.6723\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.3663 - acc: 0.6778\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 0.3541 - acc: 0.6824\n",
      "2000/2000 [==============================] - 0s 195us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 0.5883 - acc: 0.5669\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.4867 - acc: 0.6229\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.4502 - acc: 0.6385\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.4240 - acc: 0.6511\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.4076 - acc: 0.6548\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.3929 - acc: 0.6649\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.3806 - acc: 0.6719\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.3697 - acc: 0.6779\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.3619 - acc: 0.6826\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.3512 - acc: 0.6868\n",
      "2000/2000 [==============================] - 0s 215us/step\n"
     ]
    }
   ],
   "source": [
    "acc_l_without = []\n",
    "for i in range(run_times):\n",
    "    x_train, y_train, x_vali, y_vali,split_index = split_data(Xtr_cifar,y_train_o,part=0.8)\n",
    "    \n",
    "    weights = all_weights[split_index]\n",
    "    \n",
    "    model, history = train_cnn_model(x_train, y_train, x_vali, y_vali, TYPE='without_noise',\n",
    "                        lr=0.00005, decay=0.000001, batchsize=1024, epochs=10)\n",
    "\n",
    "    l,a=model.evaluate(Xts_cifar,y_test)\n",
    "    acc_l_without.append(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate average accuracy and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8789\n",
      "0.005629387178015032\n"
     ]
    }
   ],
   "source": [
    "# print(acc_l_without)\n",
    "\n",
    "print(sum(acc_l_without)/len(acc_l_without))\n",
    "print(np.array(acc_l_without).std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
