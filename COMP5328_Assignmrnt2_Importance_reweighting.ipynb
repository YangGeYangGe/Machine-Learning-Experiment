{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#------------------------------#\n",
    "# loaddta(): load data\n",
    "# standard(): normalise data\n",
    "#------------------------------#\n",
    "\n",
    "\n",
    "def loaddata():\n",
    "    dataset_mnist = np.load('mnist_dataset.npz')\n",
    "    dataset_cifar = np.load('cifar_dataset.npz')\n",
    "\n",
    "    Xtr_mnist = dataset_mnist['Xtr']\n",
    "    Str_mnist = dataset_mnist['Str']\n",
    "    Xts_mnist = dataset_mnist['Xts']\n",
    "    Yts_mnist = dataset_mnist['Yts']\n",
    "\n",
    "    Xtr_cifar = dataset_cifar['Xtr']\n",
    "    Str_cifar = dataset_cifar['Str']\n",
    "    Xts_cifar = dataset_cifar['Xts']\n",
    "    Yts_cifar = dataset_cifar['Yts']\n",
    "\n",
    "    return Xtr_mnist, Str_mnist, Xts_mnist, Yts_mnist, Xtr_cifar, Str_cifar, Xts_cifar, Yts_cifar\n",
    "\n",
    "\n",
    "def standard(x_train, x_test):\n",
    "    '''\n",
    "    Note:\n",
    "         Use the std and mean from the training data and implement\n",
    "         it to the validation data and test data\n",
    "    RETRUN:\n",
    "        Flattened and standard(Z-score normalization) training data\n",
    "        The features size is 3072 or 784\n",
    "    '''\n",
    "    std = np.std(x_train, keepdims=True)\n",
    "    mean = np.mean(x_train, keepdims=True)\n",
    "    x_train = (x_train-mean)/std\n",
    "    x_test = (x_test-mean)/std\n",
    "    return x_train, x_test\n",
    "\n",
    "#============#\n",
    "# split_data\n",
    "#============#\n",
    "\n",
    "def split_data(x_train, y_train, part=0.8):\n",
    "    '''\n",
    "    Shuffle then data and then split the data accroding to the part parameters\n",
    "    '''\n",
    "    all_index = np.arange(y_train.shape[0])\n",
    "    np.random.shuffle(all_index)\n",
    "    \n",
    "    train_data_size = int(y_train.shape[0] * part)\n",
    "    \n",
    "    train_index = all_index[0:train_data_size]\n",
    "    val_index= all_index[train_data_size:]\n",
    "    \n",
    "    \n",
    "    x_vali = x_train[val_index]\n",
    "    y_vali = y_train[val_index]\n",
    "\n",
    "    x_train = x_train[train_index]\n",
    "    y_train = y_train[train_index]\n",
    "\n",
    "    return x_train, y_train, x_vali, y_vali,train_index\n",
    "\n",
    "#==================#\n",
    "# One-hot embedding\n",
    "#==================#\n",
    "\n",
    "def one_hot_coding(y_train, y_test):\n",
    "    '''\n",
    "    Encode label to one-hot catogory\n",
    "    '''\n",
    "    y_train = to_categorical(y_train, num_classes=2)\n",
    "    y_test = to_categorical(y_test, num_classes=2)\n",
    "    return y_train, y_test\n",
    "\n",
    "def compute_acc(y_ture, y_pred):\n",
    "    acc = np.sum(y_ture == np.argmax(y_pred, axis=1))/len(y_ture)\n",
    "    return round(acc, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_cnn_model(x_train, y_train, x_vali, y_vali, TYPE='with_noise',\n",
    "                    lr=0.0001, decay=0.000001, batchsize=256, epochs=1):\n",
    "    '''\n",
    "    x_train: original data\n",
    "    y_train: one-hot encoding\n",
    "\n",
    "    Traning resnet model on label nosie data\n",
    "    Usesd for estimate the T\n",
    "    '''\n",
    "    # data is mnist\n",
    "    if x_train.shape[3] == 1:\n",
    "        model = Sequential([\n",
    "            Conv2D(64, (2, 2), use_bias=True, padding='same', activation='relu',input_shape=(28,28,1)),\n",
    "            MaxPooling2D(pool_size=(2, 2)),\n",
    "            Conv2D(64, (2, 2), use_bias=True, padding='same', activation='relu'),\n",
    "\n",
    "            Flatten(),\n",
    "            Dense(1000, activation='relu', use_bias=True),\n",
    "            Dense(500, activation='relu', use_bias=True),\n",
    "            Dense(2, activation=tf.nn.softmax)\n",
    "        ])\n",
    "        \n",
    "    # data is cifar\n",
    "    else:\n",
    "        model = Sequential([\n",
    "            Conv2D(64, (2, 2), use_bias=True, padding='same', activation='relu',input_shape=(32,32,3)),\n",
    "            MaxPooling2D(pool_size=(2, 2)),\n",
    "            Conv2D(64, (2, 2), use_bias=True, padding='same', activation='relu'),\n",
    "            Flatten(),\n",
    "            Dense(1000, activation='relu', use_bias=True),\n",
    "            Dense(500, activation='relu', use_bias=True),\n",
    "            Dense(2, activation=tf.nn.softmax)\n",
    "        ])\n",
    "\n",
    "    adm = optimizers.Adam(lr=lr, decay=decay)\n",
    "    if TYPE == 'with_noise':\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=adm,\n",
    "                      metrics=['accuracy'])\n",
    "        history = model.fit(x_train, y_train, batch_size=batchsize, \\\n",
    "                        epochs=epochs)\n",
    "    elif TYPE == 'without_noise':\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=adm,\n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "        history = model.fit(x_train, y_train, batch_size=batchsize,sample_weight = weights.flatten(), \\\n",
    "                        epochs=epochs)\n",
    "    return model, history\n",
    "\n",
    "\n",
    "rho0 = 0.2\n",
    "rho1 = 0.4\n",
    "def estimateBeta(S,prob,rho0,rho1):\n",
    "    n = len(S)\n",
    "    beta = np.zeros((n,1))\n",
    "    for i in range(n):\n",
    "        if S[i]==1:\n",
    "            beta[i] = (prob[i][1]-rho0)/((1-rho0-rho1)*prob[i][1]+1e-5)\n",
    "        else:\n",
    "            beta[i] = (prob[i][0]-rho1)/((1-rho0-rho1)*(prob[i][0])+1e-5)\n",
    "\n",
    "    return beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark of CNN without loss function correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'mnist_dataset.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9b54131e7dd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mXtr_mnist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStr_mnist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXts_mnist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYts_mnist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtr_cifar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStr_cifar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXts_cifar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYts_cifar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaddata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mXtr_mnist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mXts_mnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtr_mnist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mXts_mnist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mXtr_mnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXtr_mnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtr_mnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mXts_mnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXts_mnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXts_mnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_train_o\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_coding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStr_mnist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYts_mnist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-a87e0a1b512a>\u001b[0m in \u001b[0;36mloaddata\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloaddata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdataset_mnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mnist_dataset.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mdataset_cifar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cifar_dataset.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mnist_dataset.npz'"
     ]
    }
   ],
   "source": [
    "Xtr_mnist, Str_mnist, Xts_mnist, Yts_mnist, Xtr_cifar, Str_cifar, Xts_cifar, Yts_cifar = loaddata()\n",
    "Xtr_mnist,Xts_mnist = standard(Xtr_mnist,Xts_mnist)\n",
    "Xtr_mnist = Xtr_mnist.reshape(Xtr_mnist.shape[0], 28, 28,1)\n",
    "Xts_mnist = Xts_mnist.reshape(Xts_mnist.shape[0], 28, 28,1)\n",
    "y_train_o, y_test = one_hot_coding(Str_mnist,Yts_mnist)\n",
    "\n",
    "print('x_train {}, y_train {} \\nXts_mnist {}, y_test {}'.format(\n",
    "    Xtr_mnist.shape,y_train_o.shape,Xts_mnist.shape,y_test.shape))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 0.6669 - acc: 0.5981\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6486 - acc: 0.6206\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6369 - acc: 0.6300\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6276 - acc: 0.6305\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6199 - acc: 0.6423\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6145 - acc: 0.6505\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6098 - acc: 0.6621\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6070 - acc: 0.6636\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6044 - acc: 0.6690\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 37us/step - loss: 0.6026 - acc: 0.6736\n",
      "2000/2000 [==============================] - 0s 68us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 0.6767 - acc: 0.5802\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6531 - acc: 0.6105\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6381 - acc: 0.6292\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6262 - acc: 0.6344\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6172 - acc: 0.6424\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6114 - acc: 0.6539\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6062 - acc: 0.6640\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6034 - acc: 0.6664\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6010 - acc: 0.6737\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5994 - acc: 0.6797\n",
      "2000/2000 [==============================] - 0s 60us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 99us/step - loss: 0.6684 - acc: 0.6005\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6453 - acc: 0.6211\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6327 - acc: 0.6334\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6233 - acc: 0.6359\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6167 - acc: 0.6441\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6112 - acc: 0.6589\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6076 - acc: 0.6655\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6049 - acc: 0.6691\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6029 - acc: 0.6715\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6016 - acc: 0.6749\n",
      "2000/2000 [==============================] - 0s 79us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 101us/step - loss: 0.6762 - acc: 0.5734\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6560 - acc: 0.6040\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6422 - acc: 0.6296\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6325 - acc: 0.6314\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6248 - acc: 0.6356\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6191 - acc: 0.6419\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6151 - acc: 0.6544\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6114 - acc: 0.6562\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6086 - acc: 0.6625\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6076 - acc: 0.6645\n",
      "2000/2000 [==============================] - 0s 83us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 0.6738 - acc: 0.5929\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6500 - acc: 0.6110\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6383 - acc: 0.6269\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6288 - acc: 0.6277\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6214 - acc: 0.6430\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.6154 - acc: 0.6490\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.6117 - acc: 0.6532\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6087 - acc: 0.6646\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6070 - acc: 0.6601\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6053 - acc: 0.6639\n",
      "2000/2000 [==============================] - 0s 87us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 0.6724 - acc: 0.5955\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6495 - acc: 0.6183\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6373 - acc: 0.6276\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6269 - acc: 0.6322\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6193 - acc: 0.6374\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6137 - acc: 0.6591\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6101 - acc: 0.6584\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6071 - acc: 0.6674\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6048 - acc: 0.6717\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6030 - acc: 0.6761\n",
      "2000/2000 [==============================] - 0s 82us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 110us/step - loss: 0.6778 - acc: 0.5706\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6572 - acc: 0.5995\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6460 - acc: 0.6257\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6373 - acc: 0.6335\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6302 - acc: 0.6339\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6243 - acc: 0.6376\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6194 - acc: 0.6434\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.6156 - acc: 0.6534\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6122 - acc: 0.6522\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.6090 - acc: 0.6673\n",
      "2000/2000 [==============================] - 0s 101us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 0.6778 - acc: 0.5656\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6551 - acc: 0.6004\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6399 - acc: 0.6326\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6297 - acc: 0.6391\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6208 - acc: 0.6388\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6141 - acc: 0.6502\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6092 - acc: 0.6612\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6056 - acc: 0.6664\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6034 - acc: 0.6744\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6012 - acc: 0.6710\n",
      "2000/2000 [==============================] - 0s 107us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 0.6740 - acc: 0.5967\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6480 - acc: 0.6177\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6344 - acc: 0.6345\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6247 - acc: 0.6337\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6186 - acc: 0.6432\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.6136 - acc: 0.6596\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 0.6096 - acc: 0.6601\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6069 - acc: 0.6665\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6051 - acc: 0.6708\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6033 - acc: 0.6706\n",
      "2000/2000 [==============================] - 0s 108us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 0.6749 - acc: 0.5798\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6529 - acc: 0.6067\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6386 - acc: 0.6335\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6281 - acc: 0.6334\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6198 - acc: 0.6419\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6143 - acc: 0.6557\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6100 - acc: 0.6595\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6070 - acc: 0.6650\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6049 - acc: 0.6729\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6036 - acc: 0.6686\n",
      "2000/2000 [==============================] - 0s 115us/step\n"
     ]
    }
   ],
   "source": [
    "for_flip_rate_1_l = []\n",
    "for_flip_rate_0_l = []\n",
    "acc_l = []\n",
    "run_times = 10\n",
    "x_train_x_results = []\n",
    "for i in range(run_times):\n",
    "    x_train, y_train, x_vali, y_vali,split_index = split_data(Xtr_mnist,y_train_o,part=0.8)\n",
    "    model_noise,history_noise = train_cnn_model(x_train, y_train, x_vali, y_vali, TYPE='with_noise',\n",
    "                        lr=0.00001, decay=0.000001, batchsize=1024, epochs=10)\n",
    "    \n",
    "    train_x_result = model_noise.predict(Xtr_mnist)\n",
    "\n",
    "    for_flip_rate_1_l.append(np.min(train_x_result[:,0]))\n",
    "    for_flip_rate_0_l.append(np.min(train_x_result[:,1]))\n",
    "\n",
    "    l,a=model_noise.evaluate(Xts_mnist,y_test)\n",
    "    \n",
    "    acc_l.append(a)\n",
    "    x_train_x_results.append(train_x_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.118475705, 0.112255834, 0.10182339, 0.12633117, 0.118924625, 0.12552512, 0.1293975, 0.11892501, 0.11038665, 0.11261818]\n",
      "[0.22113787, 0.22626345, 0.2290852, 0.18226421, 0.2148817, 0.21624699, 0.23876645, 0.22864051, 0.21219683, 0.2024535]\n",
      "[0.868, 0.859, 0.8375, 0.875, 0.873, 0.8815, 0.799, 0.8755, 0.8615, 0.875]\n",
      "0.8605\n",
      "0.023692825918408277\n"
     ]
    }
   ],
   "source": [
    "print(for_flip_rate_0_l)\n",
    "print(for_flip_rate_1_l)\n",
    "print(acc_l)\n",
    "print(sum(acc_l)/len(acc_l))\n",
    "print(np.array(acc_l).std())\n",
    "# print(x_train_x_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_train_x_results)):\n",
    "    if i == 0:\n",
    "        total_x_result = np.copy(x_train_x_results[i])\n",
    "    else:\n",
    "        total_x_result += x_train_x_results[i]\n",
    "        \n",
    "train_x_result = total_x_result/(run_times)\n",
    "\n",
    "all_weights = estimateBeta(Str_mnist, train_x_result, rho0, rho1)\n",
    "for i in range(len(all_weights)):\n",
    "    if all_weights[i] < 0:\n",
    "        all_weights[i] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 0.6636 - acc: 0.5694\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6046 - acc: 0.6053\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.5550 - acc: 0.6440\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 36us/step - loss: 0.5105 - acc: 0.6555\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.4707 - acc: 0.6618\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4353 - acc: 0.6671\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4059 - acc: 0.6707\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3819 - acc: 0.6678\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3626 - acc: 0.6693\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.3485 - acc: 0.6714\n",
      "2000/2000 [==============================] - 0s 124us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 128us/step - loss: 0.6664 - acc: 0.5901\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6045 - acc: 0.6187\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.5545 - acc: 0.6392\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.5086 - acc: 0.6519\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.4679 - acc: 0.6579\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.4327 - acc: 0.6646\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 0.4031 - acc: 0.6647\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.3795 - acc: 0.6669\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3611 - acc: 0.6687\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3470 - acc: 0.6685\n",
      "2000/2000 [==============================] - 0s 119us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 132us/step - loss: 0.6584 - acc: 0.6112\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5907 - acc: 0.6326\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5365 - acc: 0.6480\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4906 - acc: 0.6576\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.4496 - acc: 0.6651\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4153 - acc: 0.6700\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.3871 - acc: 0.6697\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3653 - acc: 0.6721\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3478 - acc: 0.6719\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.3357 - acc: 0.6732\n",
      "2000/2000 [==============================] - 0s 137us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 0.6665 - acc: 0.6061\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5945 - acc: 0.6401\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.5385 - acc: 0.6502\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4915 - acc: 0.6562\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4507 - acc: 0.6636\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4158 - acc: 0.6688\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3873 - acc: 0.6697\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.3648 - acc: 0.6718\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 0.3487 - acc: 0.6708\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.3366 - acc: 0.6710\n",
      "2000/2000 [==============================] - 0s 141us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 140us/step - loss: 0.6620 - acc: 0.5602\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5919 - acc: 0.6393\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5352 - acc: 0.6548\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4881 - acc: 0.6620\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4466 - acc: 0.6644\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.4122 - acc: 0.6664\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.3843 - acc: 0.6713\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.3621 - acc: 0.6711\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3466 - acc: 0.6700\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3350 - acc: 0.6701\n",
      "2000/2000 [==============================] - 0s 140us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 143us/step - loss: 0.6511 - acc: 0.5260\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.5821 - acc: 0.6348\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5299 - acc: 0.6449\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4864 - acc: 0.6584\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.4478 - acc: 0.6652\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4148 - acc: 0.6693\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3872 - acc: 0.6724\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3647 - acc: 0.6745\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3473 - acc: 0.6754\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3347 - acc: 0.6746\n",
      "2000/2000 [==============================] - 0s 151us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 148us/step - loss: 0.6700 - acc: 0.5823\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.6055 - acc: 0.6317\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5534 - acc: 0.6379\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5089 - acc: 0.6515\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.4690 - acc: 0.6588\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.4338 - acc: 0.6651\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 0.4040 - acc: 0.6668\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3791 - acc: 0.6697\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3600 - acc: 0.6701\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3451 - acc: 0.6701\n",
      "2000/2000 [==============================] - 0s 164us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 154us/step - loss: 0.6675 - acc: 0.6139\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.6076 - acc: 0.6306\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5559 - acc: 0.6510\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.5097 - acc: 0.6605\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.4685 - acc: 0.6666\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.4323 - acc: 0.6679\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4011 - acc: 0.6709\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.3758 - acc: 0.6714\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 0.3558 - acc: 0.6714\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 36us/step - loss: 0.3413 - acc: 0.6724\n",
      "2000/2000 [==============================] - 0s 167us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 158us/step - loss: 0.6531 - acc: 0.5910\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5924 - acc: 0.6314\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.5428 - acc: 0.6423\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.5003 - acc: 0.6548\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4636 - acc: 0.6612\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.4317 - acc: 0.6641\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.4044 - acc: 0.6701\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 0.3817 - acc: 0.6680\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 35us/step - loss: 0.3639 - acc: 0.6720\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3502 - acc: 0.6700\n",
      "2000/2000 [==============================] - 0s 172us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 0.6596 - acc: 0.6255\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.5904 - acc: 0.6387\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.5345 - acc: 0.6532\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4876 - acc: 0.6590\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.4474 - acc: 0.6637\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.4133 - acc: 0.6678\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3862 - acc: 0.6705\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.3644 - acc: 0.6692\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.3480 - acc: 0.6754\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 0.3356 - acc: 0.6724\n",
      "2000/2000 [==============================] - 0s 177us/step\n"
     ]
    }
   ],
   "source": [
    "acc_l_without = []\n",
    "for i in range(run_times):\n",
    "    x_train, y_train, x_vali, y_vali,split_index = split_data(Xtr_mnist,y_train_o,part=0.8)\n",
    "    \n",
    "    weights = all_weights[split_index]\n",
    "    \n",
    "    model, history = train_cnn_model(x_train, y_train, x_vali, y_vali, TYPE='without_noise',\n",
    "                        lr=0.00001, decay=0.000001, batchsize=1024, epochs=10)\n",
    "    \n",
    "#     model.evaluate(Xts_mnist,y_test)\n",
    "    l,a=model.evaluate(Xts_mnist,y_test)\n",
    "    acc_l_without.append(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9245, 0.9235, 0.925, 0.93, 0.9215, 0.921, 0.9255, 0.9305, 0.925, 0.9235]\n",
      "0.9250000000000002\n",
      "0.002974894956128708\n"
     ]
    }
   ],
   "source": [
    "print(acc_l_without)\n",
    "print(sum(acc_l_without)/len(acc_l_without))\n",
    "print(np.array(acc_l_without).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (10000, 32, 32, 3), y_train (10000, 2) \n",
      "Xts_cifar (2000, 32, 32, 3), y_test (2000, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "Xtr_cifar,Xts_cifar = standard(Xtr_cifar,Xts_cifar)\n",
    "\n",
    "Xtr_cifar = Xtr_cifar.reshape(Xtr_cifar.shape[0], 32,32,3)\n",
    "Xts_cifar = Xts_cifar.reshape(Xts_cifar.shape[0], 32,32,3)\n",
    "\n",
    "y_train_o, y_test = one_hot_coding(Str_cifar,Yts_cifar)\n",
    "\n",
    "print('x_train {}, y_train {} \\nXts_cifar {}, y_test {}'.format(\n",
    "    Xtr_cifar.shape,y_train_o.shape,Xts_cifar.shape,y_test.shape))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 0.6597 - acc: 0.6006\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.6299 - acc: 0.6465\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.6178 - acc: 0.6591\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.6067 - acc: 0.6760\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.5975 - acc: 0.6919\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.5890 - acc: 0.6965\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.5785 - acc: 0.7090\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.5714 - acc: 0.7194\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.5586 - acc: 0.7276\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.5493 - acc: 0.7366\n",
      "2000/2000 [==============================] - 0s 220us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 0.6659 - acc: 0.6006\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.6288 - acc: 0.6533\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.6166 - acc: 0.6644\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.6048 - acc: 0.6759\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.5947 - acc: 0.6855\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.5821 - acc: 0.7059\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.5712 - acc: 0.7151\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.5598 - acc: 0.7280\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.5439 - acc: 0.7440\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.5313 - acc: 0.7489\n",
      "2000/2000 [==============================] - 0s 225us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 0.6712 - acc: 0.5971\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.6365 - acc: 0.6354\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.6214 - acc: 0.6561\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.6106 - acc: 0.6704\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.6028 - acc: 0.6800\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.5942 - acc: 0.6853\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.5873 - acc: 0.7004\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.5765 - acc: 0.7062\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.5669 - acc: 0.7163\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.5543 - acc: 0.7311\n",
      "2000/2000 [==============================] - 1s 264us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 2s 203us/step - loss: 0.6630 - acc: 0.6050\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.6310 - acc: 0.6404\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.6198 - acc: 0.6575\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.6076 - acc: 0.6744\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.5989 - acc: 0.6859\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 0.5888 - acc: 0.6963\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 0.5765 - acc: 0.7112\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.5698 - acc: 0.7150\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.5578 - acc: 0.7281\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.5440 - acc: 0.7459\n",
      "2000/2000 [==============================] - 0s 233us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 0.6709 - acc: 0.5970\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.6298 - acc: 0.6443\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 0.6150 - acc: 0.6636\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.6063 - acc: 0.6729\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.5967 - acc: 0.6830\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 0.5867 - acc: 0.6944\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.5770 - acc: 0.7041\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.5663 - acc: 0.7167\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.5554 - acc: 0.7268\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.5530 - acc: 0.7221\n",
      "2000/2000 [==============================] - 1s 260us/step\n"
     ]
    }
   ],
   "source": [
    "for_flip_rate_1_l = []\n",
    "for_flip_rate_0_l = []\n",
    "acc_l = []\n",
    "run_times = 5\n",
    "x_train_x_results = []\n",
    "for i in range(run_times):\n",
    "    x_train, y_train, x_vali, y_vali,split_index = split_data(Xtr_cifar,y_train_o,part=0.8)\n",
    "    model_noise,history_noise = train_cnn_model(x_train, y_train, x_vali, y_vali, TYPE='with_noise',\n",
    "                        lr=0.00005, decay=0.000001, batchsize=1024, epochs=10)\n",
    "    \n",
    "    train_x_result = model_noise.predict(Xtr_cifar)\n",
    "\n",
    "    for_flip_rate_1_l.append(np.min(train_x_result[:,0]))\n",
    "    for_flip_rate_0_l.append(np.min(train_x_result[:,1]))\n",
    "\n",
    "    l,a=model_noise.evaluate(Xts_cifar,y_test)\n",
    "    \n",
    "    acc_l.append(a)\n",
    "    x_train_x_results.append(train_x_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.029632399, 0.027437404, 0.03711124, 0.04075333, 0.028904041]\n",
      "[0.13930126, 0.12055889, 0.15325263, 0.14035507, 0.14672887]\n",
      "[0.836, 0.7735, 0.7765, 0.845, 0.795]\n",
      "0.8051999999999999\n",
      "0.029884109489827528\n"
     ]
    }
   ],
   "source": [
    "print(for_flip_rate_0_l)\n",
    "print(for_flip_rate_1_l)\n",
    "print(acc_l)\n",
    "\n",
    "\n",
    "print(sum(acc_l)/len(acc_l))\n",
    "print(np.array(acc_l).std())\n",
    "# print(x_train_x_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_train_x_results)):\n",
    "    if i == 0:\n",
    "        total_x_result = np.copy(x_train_x_results[i])\n",
    "    else:\n",
    "        total_x_result += x_train_x_results[i]\n",
    "        \n",
    "train_x_result = total_x_result/(run_times)\n",
    "\n",
    "all_weights = estimateBeta(Str_cifar, train_x_result, rho0, rho1)\n",
    "for i in range(len(weights)):\n",
    "    if all_weights[i] < 0:\n",
    "        all_weights[i] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 2s 216us/step - loss: 0.5679 - acc: 0.5880\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.4787 - acc: 0.6269\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.4476 - acc: 0.6441\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.4374 - acc: 0.6392\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.4177 - acc: 0.6511\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.4004 - acc: 0.6594\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.3865 - acc: 0.6643\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.3747 - acc: 0.6656\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.3640 - acc: 0.6704\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.3549 - acc: 0.6809\n",
      "2000/2000 [==============================] - 1s 278us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 0.5749 - acc: 0.5749\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 0.4670 - acc: 0.6382\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 0.4372 - acc: 0.6525\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 0.4213 - acc: 0.6497\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.4039 - acc: 0.6563\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 0.3903 - acc: 0.6674\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 0.3772 - acc: 0.6749\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.3677 - acc: 0.6765\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 0.3584 - acc: 0.6844\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 0.3561 - acc: 0.6820\n",
      "2000/2000 [==============================] - 1s 277us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.6004 - acc: 0.5781\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.4856 - acc: 0.6269\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 0.4450 - acc: 0.6400\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 0.4249 - acc: 0.6464\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.4087 - acc: 0.6549\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.3915 - acc: 0.6647\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.3797 - acc: 0.6678\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 0.3668 - acc: 0.6778\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 0.3592 - acc: 0.6819\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.3480 - acc: 0.6887\n",
      "2000/2000 [==============================] - 1s 287us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 2s 229us/step - loss: 0.5905 - acc: 0.5640\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.4888 - acc: 0.6249\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.4517 - acc: 0.6414\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.4283 - acc: 0.6495\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.4060 - acc: 0.6577\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.3918 - acc: 0.6642\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.3783 - acc: 0.6695\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.3715 - acc: 0.6793\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.3627 - acc: 0.6755\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.3578 - acc: 0.6861\n",
      "2000/2000 [==============================] - 1s 281us/step\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 2s 235us/step - loss: 0.5866 - acc: 0.5773\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.4713 - acc: 0.6346\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.4469 - acc: 0.6433\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 0.4241 - acc: 0.6459\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.4083 - acc: 0.6551\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.4013 - acc: 0.6547\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 0.3858 - acc: 0.6679\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.3776 - acc: 0.6685\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.3657 - acc: 0.6744\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.3567 - acc: 0.6780\n",
      "2000/2000 [==============================] - 1s 288us/step\n"
     ]
    }
   ],
   "source": [
    "acc_l_without = []\n",
    "for i in range(run_times):\n",
    "    x_train, y_train, x_vali, y_vali,split_index = split_data(Xtr_cifar,y_train_o,part=0.8)\n",
    "    \n",
    "    weights = all_weights[split_index]\n",
    "    \n",
    "    model, history = train_cnn_model(x_train, y_train, x_vali, y_vali, TYPE='without_noise',\n",
    "                        lr=0.00005, decay=0.000001, batchsize=1024, epochs=10)\n",
    "\n",
    "    l,a=model.evaluate(Xts_cifar,y_test)\n",
    "    acc_l_without.append(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8805, 0.8865, 0.874, 0.868, 0.883]\n",
      "0.8783999999999998\n",
      "0.00661362230551457\n"
     ]
    }
   ],
   "source": [
    "print(acc_l_without)\n",
    "print(sum(acc_l_without)/len(acc_l_without))\n",
    "print(np.array(acc_l_without).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, y_train, x_vali, y_vali = split_data1(Xtr_mnist,y_train_o,part=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
